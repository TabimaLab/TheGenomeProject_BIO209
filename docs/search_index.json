[["index.html", "The Genome Project: Excercise Guidebook Chapter 1 BIOL209 (The Genome Project) Excercise Guidebook Additional information about the professor", " The Genome Project: Excercise Guidebook Javier F. Tabima Restrepo 2025-03-27 Chapter 1 BIOL209 (The Genome Project) Excercise Guidebook Welcome to The Genome Project! This document here will be the Laboratory notebook we will use throughout the term. You will find here all the exercises and resources we will use for the course Additional information about the professor Professor Tabima Restrepo is always reachable at jtabima@clarku.edu. Office hours are on Wednesday afternoons at Larsy 223, schedule by email. "],["unix-challenge.html", "Chapter 2 UNIX Challenge", " Chapter 2 UNIX Challenge Objectives To practice and remember basic coding skills that we will use during the semester. To refresh the basics of data analysis in the command line To re familiarize with the basic formats used in computational biology Download the Synechococcus sp. WH 8101 files: Syn_WH8101.asmb.fasta, Syn_WH8101.cds.fasta and Syn_WH8101.aa.fasta from the Canvas page and answer the following questions in an R Markdown file. PLEASE: Include the code used for each question as code blocks or you will get points taken away! How many lines are in each FASTA file? How many sequences are in each FASTA file? What kind of molecule is found in each FASTA file? Why are the Syn_WH101.asmb.fasta and Syn_WH101.cds.fasta files different to each other? Provide at least five different gene/protein names from the FASTA files in question Count the occurrence of each nucleotide (A, T, C, and G) in the Whole Genome Files (WGS) of WH8101 and compare it with the nucleotide content for the Coding Sequence file of WH8101 In which of the three files can you find the genes WP_174719562.1, WP_130130567.1, and WP_130130558.1 Find the function for the genes WP_130130185.1 and WP_130130145.1 in the Syn_WH8101.aa.fasta and add them in a R markdown table How many genes with the hypothetical protein function can you find in the Syn_WH8101.aa.fasta file? Provide two more protein functions (annotations) and the number of genes associated with them How would you extract all the information related to sequence names for each file? "],["fastq-files-and-understanding-read-qualities.html", "Chapter 3 FASTQ files and understanding read qualities 3.1 Sample distribution 3.2 Basic information and creating backups 3.3 Quality Control", " Chapter 3 FASTQ files and understanding read qualities Objectives To learn about the most commonly used types of files for genome sequencing: FASTQ files To understand the information included in FASTQ files To identify good quality and bad quality reads from genome sequencing datasets Today we will explore the previously Illumina sequenced genomes of the four Synechococcus samples the Ahlgren Lab has provided us to assemble and annotate. These genomes have been sequenced using the Illumina HiSeq platform, with Paired Ends and a read length of 150bp. Based on that information, answer the following questions: How many FASTQ files per sample do you expect (one FASTQ per sample or two FASTQ per sample)? Justify your answer. Do you expect all the reads to be of the same size? Which size? 3.1 Sample distribution As mentioned in class, we have three main samples sequenced: S165 and S167. We will use S165 as an example this week and then you will have to present the results of S166 and S167 as part of your lab report. We will use Smaug for our data analyses. Smaug, or the Great Worm, is the supercomputer we use in the T lab for mycological evolution. All the data is available in Smaug, so please be careful with the data provided. To access Smaug you need to connect via http://140.232.222.14:8787/. The username to access Smaug is the same of your Clark ID, but without the @clarku.edu (i.e. My user is jtabima) The files are stored in a folder at /course_data/BIOL209/raw_illumina_data/raw_reads/SAMPLE_NAME. EVERYTHING that you will do, run or program will be stored in those folders, so please be careful and DON’T ERASE THE GENOME FILES! 3.2 Basic information and creating backups Inside of your sample folders, you should be able to find the genomes in FASTQ format. What are the genome files? Add the names here In your home folder (cd ~) create a folder called Genome_backup_SAMPLE_NAME and copy the genome FASTQ files there. We know that FASTQ files are divided by four lines per each read: Sequence information DNA sequence Spacer Quality The Sequence information header (line 1) starts with a @, followed by a set of strings. The first set of strings, between the @ and a colon (:) is my sequence identifier. Each read starts with that ID. Using that information, answer the following questions: What is your sample’s sequence ID? Can you use this information to count the number of reads per sample? Add the code below and the result. (Remember, to look at a GZ file without uncompressing it, use the zcat command) Are the number of reads between file R1 and R2 the same? Was this expected? Justify your answer. Finally, summarize your results in this table: Sample Name Sequence ID Number of Reads in R1 Number of Reads in R2 030121_38 NB551394 2649963 2649963 S165 S166 S167 3.3 Quality Control High Throughput Sequencing technologies are not perfect. They can have various types of errors and contamination. Blindly using raw sequence data for downstream analysis is risky and will lead to poor, and/or inaccurate results. Here is a very helpful paper on how to “diagnose” issues with sequence data and how to improve these problems. Reading it will help you answers the questions and gain an understanding of common problems with high-throughput sequencing: Zhou, X. and Rokas, A., 2014. Prevention, diagnosis and treatment of high‐throughput sequencing data pathologies. Molecular ecology, 23(7), pp.1679-1700. The paper is available in the Canvas page of BIOL209 as well. 3.3.1 Evaluating QC of FASTQ using FastQC To evaluate the quality of our data and to “diagnose” any problems, we will use software called FastQC. This webpage can be used to learn more about each metric presented in the FastQC output: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/ 3.3.1.1 Running FastQC Create a directory within you sample folder called QC Running FastQC is super easy, just use the following command for each of your FASTQ files /Smaug_SSD/bin/fastqc -o /your/output/directory/ sequence.file.fastq.gz Answer the following question: What are the outputs of FastQC? Download the .html files to your local machine (i.e. your computer). The files should be readable by any Internet Browser. 3.3.1.2 Analizing and describing results Once you’ve transferred the .html files to your computer, open it up in a web browser. You should see a nicely arranged page with 12 analyses (see the site for an example: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/). Each of these analyses can tell us something about the quality of our data. Answer the following questions: How many sequence reads makeup our dataset? Does this agree with your previous calculation? What is the read length of our data? Is this result expected or do we see weird results? What is the GC content of our data ? Does this match expectations? Describe the sequence quality of the files (per base sequence quality, per tile sequence quality, per sequence quality score). In general, are the forward and reverse reads of similar quality? If they differ, how do they differ? Are there any issues with the various other metrics analyzed by FastQC (i.e. the red circles with the ‘X’ on the left under the ‘Summary’ heading)? Do you notice any patterns that are puzzling or troubling with any of these analyses? Improving the Illumina Data: Outline rough plan to improve the Illumina Sequence data. You don’t need to provide specific program commands and parameters, but do take a look at some of the programs recommended by Zhou and Rokas (2019) to start thinking about how you would improve or filter the read data. By all means you can consider using other software if you find something interesting or useful. "],["laboratory-techniques-for-dna-extraction.html", "Chapter 4 Laboratory techniques for DNA extraction 4.1 Protocol using spin columns 4.2 Quantification of DNA using Qubit and Nanodrop 4.3 Analytical gel for DNA integrity and fragment size 4.4 DNA extraction report", " Chapter 4 Laboratory techniques for DNA extraction Objectives To learn about bacterial genomic DNA extraction To understand the biochemical process of cell rupture and DNA elution To identify good quality genomic DNA extractions from isolated samples We use a DNA extraction kit (Monarch Genomic DNA Purification Kit) to obtain high molecular weight genomic DNA. The protocol can be found here or in our Canvas page. Read pages 4-5 and 12-13 before coming to the lab so you can familiarize yourself with the protocol 4.1 Protocol using spin columns Harvest a maximum of up to 2 x 109 Gram-negative bacteria by centrifugation for 1 minute at&gt; 12,000 x g. Discard supernatant. Add 90 µL of cold PBS or 10 mM Tris-Cl pH 8.0 and resuspend bacterial pellet by vortexing or pipetting up and down. Add 10 µL Lysozyme solution (25 mg/ml) and vortex briefly, then add 100 µL Tissue Lysis Buffer and vortex thoroughly. Incubate at 37°C for 5 minutes or until clear. Most lysates will become fully clear, but for some bacteria a slight haze may remain. Add 10 µL Proteinase K, vortex briefly, and incubate at 56°C for a minimum of 30 minutes in a thermal mixer with agitation at full speed. At this time, go to the Analytical gel for DNA integrity and fragment size section and start your gel. Add 3 µL of RNase A to the lysate, vortex briefly, and incubate for a minimum of 5 minutes at 56°C with agitation at full speed (~1400 rpm). Add 400 µL gDNA Binding Buffer to the sample and mix thoroughly by pulse-vortexing for 5-10 seconds. Thorough mixing is essential for optimal results. Transfer the lysate/binding buffer mix (~600 µL) to a gDNA Purification Column pre-inserted into a collection tube, without touching the upper column area. Proceed immediately to Step 3. Do not reload the same column with more sample Over-exposure of the matrix to the lysed sample can cause the membrane to expand and dislodge. Avoid touching the upper column area with lysate/binding mix and avoid transferring foam that may have formed during lysis. Any material that touches the upper area of the column, including any foam, may lead to salt contamination in the eluate. Close the cap and centrifuge: first for 3 minutes at 1,000 x g to bind gDNA (no need to empty the collection tubes or remove from centrifuge) and then for 1 minute at maximum speed (&gt; 12,000 x g) to clear the membrane. Discard the flow-through and the collection tube. For optimal results, ensure that the spin column is placed in the centrifuge in the same orientation at each spin step (for example, always with the hinge pointing to the outside of the centrifuge); ensuring the liquid follows the same path through the membrane for binding and elution can slightly improve yield and consistency. Transfer column to a new collection tube and add 500 µL gDNA Wash Buffer. Close the cap and invert a few times so that the wash buffer reaches the cap. Centrifuge immediately for 1 minute at maximum speed and discard the flow through. The collection tube can be tapped on a paper towel to remove any residual buffer before resulting the next step. Inverting the spin column with wash buffer prevents salt contamination in the eluate. Reinsert the column into the collection tube. Add 500 µL gDNA Wash Bufer and close the cap. Centrifuge immediately for 1 minute at maximum speed and discard the collecting tube and flow through. Place the gDNA Purification Column in a DNase-free 1.5 ml microfuge tube (not included). Add 35- 100 µL preheated (60°C) gDNA Elution Buffer, close the cap and incubate at room temperature for 1 minute. Elution in 100 µL is recommended, but smaller volumes can be used and will result in more concentrated DNA but a reduced yield (20-25% reduction when using 35 µL). Eluting with preheated elution buffer will increase yields by ~20-40% and eliminates the need for a second elution. For applications in which a high DNA concentration is required, using a small elution volume and then re­eluting with the eluate may increase yield (~10%). The elution buffer (10 mM Tris-Cl, pH 9.0, 0.1 mM EDTA) offers strong protection against enzymatic degradation and is optimal for long term storage of DNA. However, other low-salt buffers or nuclease-free water can be used if preferred. For more details on optimizing elution, please refer to “Considerations for Elution &amp; Storage” (page 4). Centrifuge for 1 minute at maximum speed (&gt; 12,000 x g) to elute the gDNA. 4.2 Quantification of DNA using Qubit and Nanodrop To determine how much DNA we extracted, we will use Qubit and Nanodrop to quantify the amount of DNA extracted. 4.2.1 Qubit flurescence quantification Add 198 µL of 1x dsDNA HS Qubit reagent in a Qubit tube Add 2 µL of your gDNA into the qubit tube Vortex for 15 seconds and incubate for 2 minutes Go to the Qubit machine and select dsDNA &gt; 1x HS dsDNA &gt; select 1 µL &gt; Measure sample Record your DNA concentration 4.2.2 Nanodrop DNA quality Take your DNA extraction to the nanodrop room with your TA. Also, bring the gDNA Elution Buffer from the kit Follow the instructions to blank and measure your DNA Record the 260/280, 230/280 and concetration of your sample Take a picture of the extraction curve 4.3 Analytical gel for DNA integrity and fragment size Mix 0.3 g of agarose into 60 ml of 1x TAE buffer Heat the mixture in the microwave for 30s in 5s intervals until the solution is clear. Mix at every interval. Wait for the mix to cool a little and add 5 μl of SYBR SAFE/SYBR GREEN to the mix Add the mixture to the gel box with the comb in it. Ask your TA/PLA/Professor for help! Wait 20/30 mins for the gel to solidify. Add 10 μl of DNA ladder 1kb in the first and last wells of the gel Mix 3 μl of the sample with 6μl of DNA loading buffer and load the mix into one of the wells in the gel. Run the gel for 30 minutes at 95 volts. Visualize the gel in the blue light machine, tak Take a picture for your electronic notebook. 4.4 DNA extraction report In an R markdown file, include the 260/230 values, 260/280 values, the concentration and picture from the Nanodrop and the concentration from the Qubit. Also include your gel and indicate your DNA size. Discuss your results, answer these questions and add them to that discussion: Is the Nanodrop concentration higher than the Qubit? Why? What are the differences between qubit and nanodrop? Would you consider your DNA extraction to be of HMW? Did you extract enough DNA for the Nanopore sequencing of your genome? "],["improving-illumina-read-quality.html", "Chapter 5 Improving Illumina Read Quality 5.1 Checking the S166 sample and trying to clean it as best as possible 5.2 What to turn in?", " Chapter 5 Improving Illumina Read Quality Objectives To understand quality control (QC) for sequencing experiments To identify common errors in illumina sequencing To propose solutions to QC issue To learn how to use Trimmomatic 5.1 Checking the S166 sample and trying to clean it as best as possible We have looked at the raw reads from S166 and have discovered that it has some possible errors and elements that may results in a poor assembly, errors in some sequences, over represented regions of the genome or even the present of contaminants. The main ones you mentioned in class were: A high percentage of adapters in the end of the R2 sequences Reduced quality in the last 5-7 bases of the files A set of reads that are smaller than the expected read length (150bp) Reads with lower average quality than the maximum quality A GC content distribution that is bimodal So your homework is to come up with how would you clean your data based on what we learnt in class and the slides of the Short read sequencing class using Trimmomatic. To run trimmomatic in the SMAUG, modify the following template: java -jar /Smaug_SSD/bin/Trimmomatic-0.39/trimmomatic-0.39.jar PE -T 1 R1.fastq.gz R2.fastq.gz Processed_R1.fastq.gz Processed_R1.Unpaired.fastq.gz Processed_R2.fastq.gz Processed_R2.Unpaired.fastq.gz OPTIONS_FOR_TRIMMING Where the R1.fastq.gz and R2.fastq.gz are your forward and reverse FASTQ files (NB0621_05_S166_R1_001.fastq.gz and NB0621_05_S166_R2_001.fastq.gz). The OPTIONS_FOR_TRIMMING are the commands that you will use for cleaning. You can use more than one at a time, and the order matters. So, if you want to: Remove the adapters Remove all reads with less than 145 bp The command will be: java -jar /Smaug_SSD/bin/Trimmomatic-0.39/trimmomatic-0.39.jar PE -threads 1 NB0621_05_S166_R1_001.fastq.gz NB0621_05_S166_R1_001.fastq.gz Clean_S166_R1.fastq.gz Clean_S166_R1.Unpaired.fastq.gz Clean_S166_R2.fastq.gz Clean_S166_R2.Unpaired.fastq.gz ILLUMINACLIP:/Smaug_SSD/bin/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10:2:True MINLEN:145 5.2 What to turn in? How would you clean the reads. This means: For each problem, plan a strategy (number of bases, quality threshold, adapters to be removed, etc.) What commands would you use for your sequence processing pipeline For extra points that will count as one extra lab (100 points): The updated HTML of FASTQC before and after processing the reads The location of your processed reads within Smaug "],["nanopore-sequencing.html", "Chapter 6 Nanopore Sequencing 6.1 Summarizing our Nanopore sequencing", " Chapter 6 Nanopore Sequencing 6.1 Summarizing our Nanopore sequencing Within the /Smaug_SSD/BIOL209/nanopore_raw_data folder you will find a report_Nanopore_YOURSAMPLE.pdf file. Download it and open it. Answer the following questions: How many reads were generated by your Nanopore run? How many bases passed the quality filters? Based on the Read Length Histogram Basecalled Bases figure, what was your N50? Based on your Read Length Histogram Basecalled Bases, what was the length of the longest read you sequenced? Based on the Mux Scan Grouped figure, what was the maximum number of Active pores (green bars) in your run? What time? Based on the QScore figure, do you see a decrease in quality over time? Based on the output by the Run Debug Messages, what was the initial number of available pores? What was the minimum number of available pores? "],["assembly-methods-for-short-read-sequencing.html", "Chapter 7 Assembly Methods for Short Read Sequencing 7.1 Comparing between k-mers in genome assembly 7.2 Using a multi-K-mer approach to avoid manual k-mer selection 7.3 Cleaning the assembly and detecting contamination 7.4 Coverage and Taxonomy detection assay 7.5 Extracting the scaffolds of interest 7.6 2025 Results:", " Chapter 7 Assembly Methods for Short Read Sequencing Objectives To understand the algorithms and programs used for De Novo assembly To test between different K-values and determine the best value for an assembly To compare between different assembly software for short read sequencing To identify metrics used to determine assembly quality To clean contaminants from genomic assemblies Recommended: Create a folder inside of your data folder called assembly and run everything there. 7.1 Comparing between k-mers in genome assembly First, lets test different k-mers and observe how it affects the assembly. We will use SPADES, one of the best assemblers available currently, to assemble our data. Using your cleaned and raw reads, choose one of the following K values (7, 21, 47) and run SPADES by modifying the following code python3 /Smaug_SSD/bin/SPAdes-4.0.0-Linux/bin/spades.py -t 3 -k k_value -1 paired_ends_R1.fastq.gz -2 paired_ends_R2.fastq.gz -o spades_k_value Using the SMAUG scheduler slurm Since we are using so many jobs at the same time, we have to start using a scheduler. The scheduler works as a guardian of the computer that runs the jobs in the background without interfering with other processes in the computer. So, to use the scheduler, we do the following: sbatch -J name_of_the_job –wrap “command_to_be_run” -o name_of_log_file Then, check if the job is being run by using the squeue command. If the job is being run, you’ll see the job in your screen. Check the stats of your assembly by using the stats.sh program from BBMAP as such: stats.sh in=spades_k_value/scaffolds.fasta Fill the following table. Dataset K-value Number of scaffolds %GC N50 Raw reads 7 Raw reads 21 Raw reads 47 Cleaned reads 7 Cleaned reads 21 Cleaned reads 47 Answer the following questions: Define Kmer using your own words Define N50 and L90 using your own words Based on your results and the table you filled, what is the best K-mer value? 7.2 Using a multi-K-mer approach to avoid manual k-mer selection As we can tell, K-mer size affects tje assemblies. That means we will have to test multiple K-mer values to find which combination of k-mers is the best one for our assembly. Luckily, there are programs that will choose the number of optimal k-mers to be tested and will select the best assembly as a final product. Unicycler is an assembly program that doesn’t exactly that. Its a program that is specific for prokariotic organisms, so it fits our genomes perfectly. The Unicycler program is super cool as it uses a lot of different programs (such as SPADES, Racon, Pilon and others) to optimize the assemblies for bacterial/archaeal organisms. Installing Unicycler Before you can run unicycler you have to install it: conda create -n unicycler_env python=3.9 -y conda install -c bioconda unicycler conda init bash bash Run Unicycler for your clean and raw reads by modifying the following command: conda activate unicycler_env /Smaug_SSD/bin/Unicycler-0.5.1/unicycler-runner.py spades_path=/Smaug_SSD/bin/SPAdes-4.0.0-Linux/bin/spades.py -1 paired_ends_R1.fastq.gz -2 paired_ends_R2.fastq.gz -o unycycler_raw/clean After the assembly has finished, evaluate the scaffolds using stats.sh and create a table below with the results. Answer the following questions: What was the best K-mer value selected by Unicycler? What assembly was better? Any of the SPADES assemblies or the Unicycler? We can calculate the coverage for the best assembly by using bwa to map the reads to our best assembly as such: bwa index best_assembly.fasta bwa mem -t 4 best_assembly.fasta paired_ends_R1.fastq.gz paired_ends_R2.fastq.gz | samtools view -S -b | samtools sort -O bam -o mapped.bam samtools index mapped.bam Where best_assembly.fasta is your selected assembly, and your output is mapped.bam. We will learn more about bam files later on the course. And then, use the pileup.sh program to summarize the coverage results as such: pileup.sh in=mapped.bam out=stats.txt Create a table summarizing the best assembly with coverage information: Dataset Assembly software Number of reads Number of scaffolds %GC N50 L90 Average coverage 7.3 Cleaning the assembly and detecting contamination So, you think we have the best assembly? NOPE. We still have some work to do. We have a little problem with our data, and that is that these bacteria might have grown with some other species. So, chances are the DNA extraction also includes other bacteria that are not Synechococcus. How can we check this? A. We do a Sketch assay that will tell us if there is DNA from other species in our genomes B. We do a coverage and taxonomy detection assay to determine the number and type of contaminants We will be doing both in this laboratory assignment. 7.3.1 Sketch assay A Sketch assay is a method of rapidly comparing your assembly results k-mers against databases populated with different reference organisms (such as RefSeq from NCBI). The method it uses is to create a hash. A Hash is a function that converts a large object into a smaller one. Then, you compare this hashes against the hashes from the reference databases to see what your genome looks like. This means that you can see if your genome has contamination from other organisms very rapidly. Lets do a sketch assay for the assemblies: Obtain the sketch for your assembled genome and compare it against the sketch of the RefSeq datasets using sendsketch.sh as such: /Smaug_SSD/bin/bbmap/sendsketch.sh in=best_assembly.fasta refseq Check the results of the program. The columns WKID KID ANI SSU Complt Matches Unique represent the Weighted k-mer identity, K-mer identity, Average nucelotide identity, 16S identity by alignment, Genome completeness, The number of shared kmers between query and ref, The number of shared kmers between query and ref, and no other ref. So, if your result is 100% in the WKID against Synechococcus, that means that 100% of the k-mers from your assembly match the k-mers from Synechococcus. Answer the following questions: What are the GENUS with higher WKID scores? What is the WKID score of your assembly when compared to Synechococcus? Summarize you results and what they mean 7.4 Coverage and Taxonomy detection assay Before we start do this: conda deactivate &quot;${SHELL}&quot; &lt;(curl -L micro.mamba.pm/install.sh) follow the instructions and then do the following to install blobtools, the program we will use to assay for contamination. conda create -n blobtools conda activate blobtools micromamba install bioconda::blobtools Now we suspect that our sequences have a lot of environmental contaminants, but we don’t know which of the scaffolds are from our bacteria of interest or which ones are from contaminants. This is because the results from the sendsketch.sh assay are across the entire assembly.To have a good assemblythat only respresnets our species of interest we should identify and remove all scaffolds that are not matching the taxa we are interested in. To do this, we use blobtools. Blobtools determines the taxonomic assignment and the coverage for every single scaffold in your assembly! Blobtools needs three files: Your assembly in FASTA format Your BAM file with mapped reads to your best assembly (mapped.bam). This will assess the coverage for each scaffold, as contaminant scaffolds tend to have a different coverage than scaffolds from our species of interest. A NCBI BLAST table where each scaffold is compared to the NCBI database. This will tell us the taxonomic assignment of each scaffold. Then, blobtools will put all the information together and provide us with two main results: A list of the taxonomic assignment and coverage of each scaffold A plot that shows the taxonomic assignment and coverage of each scaffold Let’s, then, use blobplots and create the taxonomic and coverage assay for our assembled scaffolds. As of now, we have two files: - Your assembly in FASTA format - Your BAM file with mapped reads to your best assembly (mapped.bam) from step 8 in this lab We are missing the BLAST taxonomic assignments Do a BLAST search of our scaffolds against NCBI’s nt database by first creating a new folder called blobtools and running everything there: mkdir blobtools cd blobtools blastn -num_threads 4 -task megablast -query best_assembly.fasta -db /course_data/blastdb/nt -outfmt &#39;6 qseqid staxids bitscore std&#39; -max_target_seqs 1 -max_hsps 1 -evalue 1e-25 -out best_assembly.blast Where best_assembly.fasta is you assembly file, -db is the nt database from NCBI Answer the following questions using your knowledge from MBB 101 What blast program are we using and what does it mean? What is the nt database? What do the -max_target_seqs and -max_hsps flags mean? What is the e-value used as threshold? What does e-value mean? Now, after BLAST is done we have all the elements to run blobltools as such: blobtools create -i best_assembly.fasta -b mapped.bam -t best_assembly.blast -o blobltools_out --db /Smaug_SSD/bin/blobtools/data/nodesDB.txt Where -i is your assembly, -b is the BAM file of reads mapped to the assembly, -t is the BLAST output, and -o is the output name for the run. Blobtools needs a two additional steps in order to visualize and plot the results as this: blobtools view -i blobltools_out.blobDB.json; blobtools plot -i blobltools_out.blobDB.json Let’s download the plots in the .png files. the blobtools_out.blobDB.json.bestsum.phylum.p8.span.100.blobplot.read_cov.bam0.png shows the percentage of all taxonomic assignments in our scaffold. Answer the following questions: Include the image. What is the taxonomical unit with the highest percentage of scaffolds in our assembly? What is the percentage of scaffolds with assignment to Synechococcus? The blobtools_out.blobDB.json.bestsum.phylum.p8.span.100.blobplot.bam0.png file is a blobplot that shows three main things: In Top: A length (span) vs. GC content plot that shows the size of each scaffold versus its GC content. The colors represent the taxonomic assignment In the middle: A Bubbleplot of coverage vs GC content for each scaffold. Larger the circle, larger the scaffold. The colors represent the taxonomic assignment In the right: A coverage vs. length (span) plot that shows the coverage of each scaffold versus its length The colors represent the taxonomic assignment What this plot shows us is the differences between all the scaffolds assembled. We can see, here, how the contaminants look versus our desired sequence. Answer the following questions: Include the image. What can you say about this plot? Back in the cluster, check the blobtools_out.blobDB.json.bestsum.phylum.p8.span.100.blobplot.stats.txt file. That file contains a summary of all the stats from blobtools. Copy and paste the results into this page: https://www.tablesgenerator.com/markdown_tables and copy and paste the result below: Add table here Here’s what each column means: name: Best taxonomic hit colour: Color used for the plots count_visible: Number of scaffolds associated with the taxonomic hit count_visible_perc: Percentage of assembly scaffolds associated with the taxonomic hit span_visible: Number of bases associated with the taxonomic hit span_visible_perc: Percentage of total bases associated with the taxonomic hit n50: N50 per taxonomic hi gc_mean: Mean GC content per taxonomic hit gc_std: Standard deviation of GC content per taxonomic hit bam0_mean: Mean coverage per taxonomic hit bam0_std: Standard deviation of coverage per taxonomic hit bam0_read_map: Number of reads mapped to the taxonomic hit bam0_read_map_p: Percentage of reads mapped to the taxonomic hit Answer the following questions: What is the most represented taxonomic hit in your dataset? How did you come to that conclusion? What about our Cyanobacteria? Can you summarize the results in your own words? Finally, to clean the assembly we will extract the scaffolds that match Cyanobacteria. The information of the taxonomic assignment for each scaffold is found in the blobtools_out.blobDB.table.txt. Open the first 20 lines of the file using the program of your choice. Here’s what each column mean: name: Name of the sequence length: Total length of the sequence, i.e. count(A, G, C, T, N) GC: GC content percentage of the sequence, i.e. count(G, C)/count(A, G, C, T) N: Number of N’s in the sequence, i.e. count(N) bam0: Coverage from bam0 (see main header for filename) phylum.t.6%s: The assigned taxonomy of the sequence at the taxonomic rank of “phylum” under the tax-rule “best-sum” phylum.s.7%s: The sum of scores for the taxonomy of the sequence at the taxonomic rank of “phylum” under the tax-rule “best-sum” phylum.c.8: The c-index for the taxonomy of the sequence at the taxonomic rank of “phylum” under the tax-rule “best-sum” Answer the following question: How can you use this file to extract the scaffolds of interest? 7.5 Extracting the scaffolds of interest The final step of our assembly is to extract all the scaffolds that have cyanobacterial associations. We can extract those scaffolds by using the blobtools seqfilter program. This program can be ran as such: blobtools seqfilter -i FASTA -l LIST -o PREFIX &gt; Cynaobacteria_scaffolds.fasta blobtools seqfilter -i assembly.fasta -l bplot/cyano_scaffs.txt -o Cyano_scaffold Where -i is your assembly, -l is the list of sequence names you want to extract, and -o is the new names for the new assembly. So, we have the -i and -o elements. Answer the following question: Using the command line tools we have learnt before, how do we extract everything that is Cyanobacteria from the blobtools_out.blobDB.table.txt to create the required file by -l? (Hint, use the grep and cut commands in a pipe.) This is an example of how the file should look like: $ head list_of_cyano_scaffolds.txt 4 8 21 30 34 Run the blobtools seqfilter command and extract the cyanobacterial reads. Then use the stats.sh command in the new extracted files (your final assembly). What are the stats? Create a table below. Finally, how can you check rapidly that there is no contamination in the new assembly? (Hint: Use a program that compares sketches). Paste the result below. Summarize your final assembly in your own words: 7.6 2025 Results: S165 S167 6167 S166 Cyano scaff # Cyano gen length Cyano scaff # Cyano gen length Cyano scaff # Cyano gen length Cyano scaff # Cyano gen length Spades 136 5.25 MB 621 2.47 852 2.85 32 2.36 Unicycler 67 6.16 MB 29 2.37 62 2.52 31 2.37 Observations Inconclusive data: Too many scaffolds and genome too long. Unicycler is better: Fewer scaffolds, less contaminants, Shorter length? Unicycler is better: Fewer scaffolds, less contaminants, Pretty similar. Unicycler may be marginally better BUT if you only extract cyanobacteria, doesnt really matter. Strategies Alternative: Long read sequencing Decent enough. Annotate! Decent enough. Annotate! Decent enough. Annotate! "],["hybrid-assembly-methods-for-short-and-long-reads.html", "Chapter 8 Hybrid Assembly Methods for Short and Long Reads 8.1 Hybrid assembly using Unicycler", " Chapter 8 Hybrid Assembly Methods for Short and Long Reads Objectives Understand the advantages of hybrid genome assemblies Perform hybrid genome assembly using Unicycler Evaluate and compare assembly quality metrics In the modern era of cheaper sequencing and easily accessible genome sequencers, we can also include long reads in our assembly. The long reads will provide the assemblers with longer regions in which the different fragments can anchor and connect, creating, in theory, longer scaffolds and a more complete assembly. Conversely, these long reads usually have lower quality than short reads and they are less abundant, so using them by themselves may lead to an assembly with a larger percentage or errors than, for example, an assembly with only short reads. So, the best of both worlds can be combined into what we call hybrid assemblies. Hybrid assemblies use both short and long reads to produce a high quality assembly (compred to an assembly of long reads only) with longer and fewer scaffolds than assemblies done with only short reads. To perform hybrid genome assemblies, we need two main files: Our Illumina reads (Choose between raw reads or cleaned reads) Our Nanopore reads (Located at the /course_data/BIOL209/nanopore_raw_data/ folder) 8.1 Hybrid assembly using Unicycler The program unicycler is combination of programs (also called a pipeline) to perform a hybrid assembly. Usually, a manual hybrid assembly (i.e. using each program separately) entails the following steps: Spades to make the short reads assemblies miniasm and racon to bridge between the scaffolds from the short read assemblies bwa or bowtie to map the reads to the new elongated scaffolds for error control pilon to correct the errors detected by the previous read mapping step However, for prokariotic organisms as the ones we are trying to assemble, our Unicycler program will do everything mentioned above and more, so hopefully we will have the best assambly possible using the highest amount of data. To run Unicycler with the long read and short read datasets, modify the following command with your FASTQ reads: /Smaug_SSD/bin/Unicycler-0.5.1/unicycler-runner.py --spades_path=/Smaug_SSD/bin/SPAdes-4.0.0-Linux/bin/spades.py --racon_path=/Smaug_SSD/bin/racon/build/bin/racon -1 illumina_reads_forward.fastq.gz -2 illumina_reads_reverse.fastq.gz -l /course_data/BIOL209/nanopore_raw_data/longread.fastq.gz -o unicycler_hyb_S167 -t 8 Where: --spades_path= is where SPADes binary is located at --racon_path= is the location for RACON, a program that ‘generates genomic consensus which is of similar or better quality compared to the output generated by assembly methods which employ both error correction and consensus steps, while providing a speedup of several times compared to those methods. It supports data produced by both Pacific Biosciences and Oxford Nanopore Technologies.’ -1 and -2 are the illumina reads -l are the nanopore/PacBio reads -t are the number of computing processors to use Let Unicycler run. Answer the following questions: Fill the following table using the information from the hybrid assembly and your previous best short read assembly (Hint: use the output from stats.sh): Source Number of scaffolds %GC N50 L90 Short read assembly Hybrid assembly Use a Sketch assay to determine the taxonomic markup of your long read assembly and answer the following questions for the hybrid assembly: What are the GENUS with higher WKID scores? What is the WKID score of your assembly when compared to Synechococcus? Summarize you results and what they mean Use blobtools to identify the taxonomic markup of your long read assembly and answer the following questions (Only map your short reads to the new assembly. Remember to use the bwa index long_read_assembly.fasta before you map the reads using bwa mem) Include the image here: What is the taxonomical unit with the highest percentage of scaffolds in our assembly? What is the percentage of scaffolds with assignment to Synechococcus? Finally, extract all the contigs/scaffolds with Cyanobacteria ID. Save these scaffolds in a file called final_cyano_assembly.fasta Map the short reads to the final_cyano_assembly.fasta using BWA. NOTE: Please remember to do the bwa index final_cyano_assembly.fasta code Use the pileup.sh program to summarize the coverage results. Save the number of mapped reads and the percentage of reads mapped. Use the pileup.sh program to summarize the coverage results and fill the following table. Dataset Number of reads Percentage of mapped reads Average coverage Short read assembly Hybrid assembly Did your long reads help improve the assembly? Justify your answer. "],["gene-prediction-methods.html", "Chapter 9 Gene Prediction Methods 9.1 Installing prokka 9.2 Annotating the genomes using prokka", " Chapter 9 Gene Prediction Methods Objectives Annotate assembled genomes Interpret and summarize annotation output files After assembling the genome, we need to identify which regions are genes. This is called gene annotation or gene prediction, and uses a combination of ab-initio approaches with homology-based searches to predict and annotate the genes. We will use prokka, a state-of-the-art program for prokariotic genome annotation. This program uses both approaches to predict genes. 9.1 Installing prokka #*# Installing miniconda Before we can start installing prokka, we need to create a computational environment for it to be able to run correctly. Miniconda provides that environment by creating a python instance that groups all the requirements for these programs to run. bash /Smaug_SSD/bin/Miniconda3-latest-Linux-x86_64.sh Say yes to all the prompts, and let miniconda install. Then, close your window and reconnect to Smaug. 9.1.1 Installing the prokka environment and program After logging back in, we need to install Prokka using miniconda as following: bash conda init bash log out and log in again, and then: bash conda create -n prokka_env -c conda-forge -c bioconda prokka conda activate prokka_env We can finally run prokka. Test it by running prokka A long prompt shoud appear. 9.2 Annotating the genomes using prokka Usually, these computationally complex programs are very long and cumbersome to run, but the develkopers of prokka have made it super easy to run. Within your sample folder, run the following code: prokka --dbdir /home/jtabima/micromamba/envs/prokka_env/db --genus Synechoccocus --strain the_name_of_your_strain --outdir your_sample_name_annotation --prefix your_sample_name your_assembly.fasta And let it run! 9.2.1 Summarizing the results from prokka After prokka is done, the outputs should be in your your_sample_name_annotation folder. Using the head command, answer the following question: Check the .fna, .fsa, .ffn, .faa, .gff, .tbl, .tsv, and .gbk files and explain what are their contents below: .fna file: .fsa file: .ffn file: .faa file: .gff file: .tbl file: .tsv file: .gbk file: Hint: Check the Prokka manual Using your knowledge on the command line, can you count the number of predicted proteins, and all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA) Open the .txt file. Answer the following questions: How many CDS are found? Do they match your counts from the previous step? Define rRNA, tRNA and tmRNA (cite your references please) and present the counts of each RNA type. Using the head command, describe the columns of your .gff file. Use the following code to create a summary table of your functional annotations: cut -f 7 your_sample_name.tsv | sort | uniq -c | sort -nr &gt; gene_counts.txt Open the gene_counts.txt file and answer the following questions: What is the most abundant gene prediction? What is the most abundant gene prediction that is not a hypothetical/predicted protein? What are the most common five gene annotations in your genome? Finally, select three random hypothetical proteins from your faa file. Run them in InterPro Scan (https://www.ebi.ac.uk/interpro/search/sequence/). Answer the following: Did any of these proteins have an annotation? Add it below. Add the path of your faa file here: "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
